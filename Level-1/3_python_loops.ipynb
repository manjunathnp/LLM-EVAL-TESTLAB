{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbc0b3bd",
   "metadata": {},
   "source": [
    "## Loops (for, while)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a136cbd8",
   "metadata": {},
   "source": [
    "### **1. Loops (for, while)**\n",
    "\n",
    "Loops allow you to **repeat a block of code** multiple times.\n",
    "\n",
    "- **`for` loop** → Iterates over a sequence (list, tuple, dictionary, set, string).\n",
    "- **`while` loop** → Runs as long as a condition is true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f3d9042",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "Num-1\n",
      "Num-2\n",
      "Num-3\n",
      "Num-4\n",
      "Num-5\n"
     ]
    }
   ],
   "source": [
    "# for-loop \n",
    "for i in range(3):\n",
    "    print(i)\n",
    "\n",
    "for num in range(5):\n",
    "    print(f\"Num-{num+1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3db6a59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1\n",
      "Run 2\n",
      "Run 3\n"
     ]
    }
   ],
   "source": [
    "# while loop\n",
    "count = 0\n",
    "while count < 3:\n",
    "    print(f\"Run {count+1}\")\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be3080d7",
   "metadata": {},
   "source": [
    "### **2. Explanation Specific to AI Evaluation/Testing**\n",
    "\n",
    "In AI evaluation:\n",
    "\n",
    "- **for loops** → iterate over prompts, outputs, or test cases to run evaluations.\n",
    "- **while loops** → useful for retrying model calls until a valid output is received.\n",
    "\n",
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3455d1df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating: What is AI?\n",
      "Evaluating: Define ML\n",
      "Evaluating: What is the captial of Australia\n"
     ]
    }
   ],
   "source": [
    "# Loop through multiple test cases\n",
    "prompts = [\n",
    "    \"What is AI?\",\n",
    "    \"Define ML\",\n",
    "    \"What is the captial of Australia\"\n",
    "]\n",
    "\n",
    "for prompt in prompts:\n",
    "    print(f\"Evaluating: {prompt}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9236490",
   "metadata": {},
   "source": [
    "### **3. AI Evaluation/Testing Exercise**\n",
    "\n",
    "**Goal:**\n",
    "\n",
    "Iterate through test cases, run a mock hallucination score check, and print pass/fail.\n",
    "\n",
    "**Code:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d26ed62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: What is AI? | Pass: True\n",
      "Prompt: Define machine learning | Pass: True\n",
      "Prompt: Capital of Japan | Pass: False\n"
     ]
    }
   ],
   "source": [
    "# Mock Test Data\n",
    "eval_results = [\n",
    "    {\"prompt\": \"What is AI?\", \n",
    "     \"hallucination_score\":0.05\n",
    "    },\n",
    "    {\"prompt\": \"Define machine learning\", \n",
    "     \"hallucination_score\": 0.1\n",
    "    },\n",
    "    {\"prompt\": \"Capital of Japan\",\n",
    "     \"hallucination_score\": 0.25\n",
    "    }\n",
    "]\n",
    "# Threshold\n",
    "hallucination_threshold = 0.2\n",
    "\n",
    "# Using a for loop\n",
    "for case in eval_results:\n",
    "    passes_test = case[\"hallucination_score\"] <= hallucination_threshold\n",
    "    print(f\"Prompt: {case['prompt']} | Pass: {passes_test}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a32bd8f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempt 1 | Score: 0.41\n",
      "Attempt 2 | Score: 0.2\n"
     ]
    }
   ],
   "source": [
    "# Using a while loop for retrying (mocked example)\n",
    "import random\n",
    "prompt = \"What is AI?\"\n",
    "score = 1.0\n",
    "attempts = 0\n",
    "\n",
    "# Threshold\n",
    "hallucination_threshold = 0.2\n",
    "\n",
    "while score > hallucination_threshold and attempts < 3:\n",
    "    score = round(random.uniform(0, 1), 2)  # mock score generation\n",
    "    attempts += 1\n",
    "    print(f\"Attempt {attempts} | Score: {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ccd887",
   "metadata": {},
   "source": [
    "### **4. DeepEval/RAGAs Context**\n",
    "\n",
    "When integrating with **DeepEval** or **RAGAs**, loops are used to:\n",
    "\n",
    "- Feed multiple prompt-output pairs into metrics.\n",
    "- Aggregate results into a single report.\n",
    "\n",
    "Example with DeepEval:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e670d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install deepeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4ad746",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Enter you OPENAI API KEY to execute below block\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"sk-proj-\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f48f367",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 1.0 | Pass: True\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 1.0 | Pass: True\n"
     ]
    }
   ],
   "source": [
    "from deepeval.metrics import FaithfulnessMetric\n",
    "from deepeval.test_case import LLMTestCase\n",
    "\n",
    "faithfulness_metric = FaithfulnessMetric(threshold=0.8)\n",
    "\n",
    "test_cases = [\n",
    "    LLMTestCase(input=\"What is AI?\", actual_output=\"AI stands for Artificial Intelligence\", retrieval_context=[\"AI stands for Artificial Intelligence\"]),\n",
    "    LLMTestCase(input=\"Capital of Japan?\", actual_output=\"Tokyo\", retrieval_context=[\"Tokyo is the capital of Japan\"])\n",
    "]\n",
    "\n",
    "for case in test_cases:\n",
    "    faithfulness_metric.measure(case)\n",
    "    print(f\"Score: {faithfulness_metric.score} | Pass: {faithfulness_metric.is_successful()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e666debb",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "testlab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
