{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3651e4c",
   "metadata": {},
   "source": [
    "### **1. Lists, Tuples, Sets, Dictionaries, and Frozensets**\n",
    "\n",
    "Python collections can be classified by **order** (whether elements keep insertion order) and **mutability** (whether you can change them after creation):\n",
    "| Order | Mutable | Immutable |\n",
    "| --- | --- | --- |\n",
    "| Ordered | List | Tuple |\n",
    "| Unordered | Set, Dictionary | Frozenset |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220ca209",
   "metadata": {},
   "source": [
    "### **2. Generic Python Explanation & Examples**\n",
    "\n",
    "#### **2.1 Ordered Mutable – List**\n",
    "- Can be changed after creation.\n",
    "- Keeps elements in the order you add them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "029c1375",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Responses ['Yes', 'No', 'May be', 'Not Sure']\n"
     ]
    }
   ],
   "source": [
    "# List\n",
    "responses = [\"Yes\", \"No\", \"May be\"]\n",
    "\n",
    "responses.append(\"Not Sure\")\n",
    "\n",
    "print(\"Responses\", responses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78526d1a",
   "metadata": {},
   "source": [
    "#### **2.2 Ordered Immutable – Tuple**\n",
    "- Fixed data once created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8b363187",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10.5, 20.1)\n"
     ]
    }
   ],
   "source": [
    "# Tuple\n",
    "coordinates = (10.5, 20.1)\n",
    "\n",
    "# coordinates[0] = 11.5 # Error: 'tuple' object does not support item assignment\n",
    "\n",
    "print(coordinates)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1b9799",
   "metadata": {},
   "source": [
    "### **2.3 Unordered Mutable – Set**\n",
    "- Unique items only, duplicates removed automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e14206de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models:  {'gpt-5', 'mistral', 'local_llm', 'deepseek-r1'}\n"
     ]
    }
   ],
   "source": [
    "# Set\n",
    "models = {\"gpt-5\", \"mistral\", \"deepseek-r1\"}\n",
    "models.add('local_llm')\n",
    "print(\"Models: \", models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "56ccff07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models:  {'gpt-5', 'mistral', 'local_llm', 'deepseek-r1'}\n"
     ]
    }
   ],
   "source": [
    "models.add(\"gpt-5\") # ignore duplicates\n",
    "print(\"Models: \", models) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f162d363",
   "metadata": {},
   "source": [
    "### **2.4 Unordered Mutable – Dictionary**\n",
    "- Key-value storage, keys must be unique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3c212e5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics:  {'faithfulness': 0.95, 'relevance': 0.89}\n",
      "Updated Metrics:  {'faithfulness': 0.95, 'relevance': 0.91}\n"
     ]
    }
   ],
   "source": [
    "# Dictonary\n",
    "metrics = {\"faithfulness\": 0.95, \"relevance\": 0.89}\n",
    "print(\"Metrics: \", metrics)\n",
    "\n",
    "# Update \"relevance\"\n",
    "metrics[\"relevance\"] = 0.91\n",
    "print(\"Updated Metrics: \", metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2b0abc",
   "metadata": {},
   "source": [
    "### **2.5 Unordered Immutable – Frozenset**\n",
    "- Set that cannot be changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e13b0deb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allowed Metrics:  frozenset({'relevance', 'faithfulness', 'bias'})\n"
     ]
    }
   ],
   "source": [
    "allowed_metrics = frozenset([\"faithfulness\", \"relevance\", \"bias\"])\n",
    "\n",
    "# try to update 'allowed_metrics' \n",
    "#allowed_metrics[0] = 'hallucination' # Error: 'frozenset' object does not support item assignment\n",
    "\n",
    "print(\"Allowed Metrics: \",allowed_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca5bf2a",
   "metadata": {},
   "source": [
    "### **3. Explanation Specific to AI Evaluation/Testing**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b2d23a",
   "metadata": {},
   "source": [
    "### **Lists**\n",
    "- Store multiple model outputs or evaluation scores for a single prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8abd8c82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hallucination Scores:  [0.05, 0.1, 0.0, 0.07]\n"
     ]
    }
   ],
   "source": [
    "hallucination_scores = [0.05, 0.1, 0.0, 0.07]\n",
    "print(\"Hallucination Scores: \", hallucination_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19c4388",
   "metadata": {},
   "source": [
    "### **Tuples**\n",
    "- Store fixed configuration values like thresholds `(min_score, max_score)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "85767cf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum Score:  0.8\n",
      "Maximum Score:  1.0\n"
     ]
    }
   ],
   "source": [
    "score_range = (0.8, 1.0)\n",
    "print(\"Minimum Score: \", score_range[0])\n",
    "print(\"Maximum Score: \", score_range[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed04a88",
   "metadata": {},
   "source": [
    "### **Sets**\n",
    "- Store unique prompts tested to avoid duplicate evaluations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "27367656",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Prompt:  {'What is AI?', 'Define machine learning'}\n"
     ]
    }
   ],
   "source": [
    "test_prompts = {\"What is AI?\", \"Define machine learning\"}\n",
    "print(\"Test Prompt: \",test_prompts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e8708f",
   "metadata": {},
   "source": [
    "### **Dictionaries**\n",
    "- Map prompt IDs to multiple evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ea3a30ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'TC_001': {'faithfulness': 0.8, 'bias': 0.2}, 'TC_002': {'faithfulness': 0.9, 'bias': 0.1}, 'TC_003': {'faithfulness': 0.7, 'bias': 0.5}}\n",
      "\n",
      "\n",
      "{'TC_001': {'bias': 0.2, 'faithfulness': 0.8},\n",
      " 'TC_002': {'bias': 0.1, 'faithfulness': 0.9},\n",
      " 'TC_003': {'bias': 0.5, 'faithfulness': 0.7}}\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "\n",
    "eval_results = {\n",
    "    \"TC_001\" : {\"faithfulness\": 0.8, \"bias\": 0.2},\n",
    "    \"TC_002\" : {\"faithfulness\": 0.9, \"bias\": 0.1},\n",
    "    \"TC_003\" : {\"faithfulness\": 0.7, \"bias\": 0.5}\n",
    "}\n",
    "\n",
    "print(eval_results)\n",
    "print(\"\\n\")\n",
    "pprint.pprint(eval_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd56d28c",
   "metadata": {},
   "source": [
    "### **Frozensets**\n",
    "- Store fixed allowed metric names to prevent accidental modification in large-scale evaluation pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0ad15cb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Required Metrics:  frozenset({'relevance', 'faithfulness'})\n"
     ]
    }
   ],
   "source": [
    "required_metrics = frozenset([\"faithfulness\", \"relevance\"])\n",
    "# required_metrics[1] = \"bias\" # Error: 'frozenset' object does not support item assignment\n",
    "\n",
    "print(\"Required Metrics: \", required_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff8e3fd",
   "metadata": {},
   "source": [
    "### **4. AI Evaluation/Testing Exercise**\n",
    "\n",
    "**Goal:**\n",
    "\n",
    "Store evaluation results for multiple prompts, ensure no duplicate prompts, and check if all metrics are valid.\n",
    "\n",
    "**Code-1:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "74c7ba6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All metrics valid: True\n"
     ]
    }
   ],
   "source": [
    "# Example scores for one question\n",
    "metrics = {\"faithfulness\": 0.95, \"relevance\": 0.9}\n",
    "\n",
    "# Allowed metric names\n",
    "allowed_metrics = [\"faithfulness\", \"relevance\"]\n",
    "\n",
    "# Check if all metrics are allowed\n",
    "all_metrics_ok = True\n",
    "\n",
    "for metric in metrics:  # loop through each metric name\n",
    "    if metric not in allowed_metrics:\n",
    "        all_metrics_ok = False\n",
    "\n",
    "print(\"All metrics valid:\", all_metrics_ok)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36bede84",
   "metadata": {},
   "source": [
    "**Code-2:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "700a66d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Questions (set): {'What is AI?', 'Define machine learning'}\n",
      "Example tuple: ('AI', 2025)\n",
      "Allowed metrics (frozenset): frozenset({'relevance', 'faithfulness'})\n",
      "Invalid metrics found (list): []\n",
      "All metrics valid: True\n"
     ]
    }
   ],
   "source": [
    "# Set: unique questions (no duplicates allowed)\n",
    "questions = {\"What is AI?\", \"Define machine learning\"}\n",
    "\n",
    "# Dictionary: scores for each question\n",
    "scores = {\n",
    "    \"What is AI?\": {\"faithfulness\": 0.95, \"relevance\": 0.9},\n",
    "    \"Define machine learning\": {\"faithfulness\": 0.92, \"relevance\": 0.88}\n",
    "}\n",
    "\n",
    "# Frozenset: allowed metrics (like a set, but cannot be changed)\n",
    "allowed_metrics = frozenset([\"faithfulness\", \"relevance\"])\n",
    "\n",
    "# Tuple: example of fixed order data\n",
    "example_tuple = (\"AI\", 2025)\n",
    "\n",
    "# List: will store any invalid metric names found\n",
    "invalid_metrics = []\n",
    "\n",
    "# Loop through each question in the set\n",
    "for question in questions:\n",
    "    # Get the metrics dictionary for the current question\n",
    "    metrics = scores[question]\n",
    "    \n",
    "    # Loop through each metric name (dictionary keys)\n",
    "    for metric in metrics:\n",
    "        if metric not in allowed_metrics:  # Compare with frozenset\n",
    "            invalid_metrics.append(metric)  # Add to list if not allowed\n",
    "\n",
    "# Output results\n",
    "print(\"Questions (set):\", questions)\n",
    "print(\"Example tuple:\", example_tuple)\n",
    "print(\"Allowed metrics (frozenset):\", allowed_metrics)\n",
    "print(\"Invalid metrics found (list):\", invalid_metrics)\n",
    "print(\"All metrics valid:\", len(invalid_metrics) == 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634a8db8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "testlab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
